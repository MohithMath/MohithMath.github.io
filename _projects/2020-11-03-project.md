---
title: "Memorization in Neural Networks: Does the Loss Function matter?"
collection: projects
type: "Research project"
permalink: /projects/2020-11-03-project
excerpt: 'We look at the nature of robustness provided by symmetric loss functions against label noise and how they resist memorization in neural networks.'
venue: "LSM Lab, Indian Institute of Science"
date: 2020-11-03
location: "Bengaluru, India"
---

> To be updated.
> Work under peer-review

{% include toc icon="cog" title="Table of Contents" %}

Deep Neural Networks, often owing to the overparameterization, have shown to be capable of exactly memorizing even randomly-labelled data. Empirical studies have also shown that none of the standard regularization techniques mitigate such overfitting. We investigate whether choice of loss function can affect this memorization. We empirically show, with benchmark data sets MNIST and CIFAR-10, that a symmetric loss function as opposed to either cross entropy or squared error loss results in significant improvement in the ability of the network to resist such overfitting. We then provide a formal definition for robustness to memorization and provide theoretical explanation as to why the symmetric losses provide this robustness. Our results clearly bring out the role loss functions alone can play in this phenomenon of memorization.

# Defining Memorization
Only recently have there been attempts at formally charaterizing what *memorization* means rigorously. Arpit et al., D2L, LIMIT and others in the community have some proxies for measuring the degree of *memorization* but Feldman et al., for the first time, try to understand memorization theoretically. In our work, we too provide a definition for *memorization* (More in Section ?).

# Role of Loss Function in resisting *memorization*
