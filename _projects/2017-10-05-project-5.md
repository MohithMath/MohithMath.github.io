---
title: "Support Recovery for Orthogonal Matching Pursuit: Upper and Lower bounds"
collection: projects
type: "Lab project"
permalink: /projects/2018-10-05-project-5
venue: "Thirty-second Conference on Neural Information Processing Systems (NIPS) 2018"
date: 2018-10-05
location: "Montr√©al, Canada"
---

{% include_relative macros/macros_project5.md %}We study the problem of Sparse Regression, a landmark problem in the domain of Machine Learning, Statistics and Optimization which finds numerous applications in Compressed Sensing, model compression, high dimensional statistics, bio-informatics, [resource constrained machine learning](https://www.microsoft.com/en-us/research/project/resource-efficient-ml-for-the-edge-and-endpoint-iot-devices/) and more. Sparse Regression can be written down as an optimization problem as

$$
\xbar = \argmin\limits_{\lznorm{\x}\leq \sstar} f(\x)
$$

where $$\x\in\Rd{d}$$, $$s^* \leq d$$, the ambient dimension and $$\lznorm{\cdot}$$ is the $$\ell_0$$ norm that denotes the number of non-zero elements of its arguments.

Our focus has been on the linear version of the problem where given $$n$$ samples of $$d$$ dimensional data $$\A\in\Rd{n\times d}$$ and a response vector $$\y\in\Rd{d}$$, we want to recover a $$\sstar$$-sparse vector $$\xbar$$ that minimizes the squared loss function $$\esqnorm{\A\x-\y}$$. However, this problem can be shown to be NP-hard via a [reduction to 3-set cover](https://pdfs.semanticscholar.org/f629/5fd69d76d606f66cc15f58767a8161d60335.pdf), nonetheless this problem has been studies under a variety of assumptions such as the ones below
1. **Mutual Incoherence** - Implies that the $$\abs{\inner{\A_i,A_j}}$$ should be bounded by some small positive real number $$M$$ for all $$i\neq j$$. In specific it can be shown OMP (which we will discuss in detail below) can recover $$\xbar$$ in ecactly $$\star$$ steps if $$M\leq \inv{2\sstar-1}$$. Refer [Theorem 5.14, Foucart and Rauhut](http://www.cis.pku.edu.cn/faculty/vision/zlin/A%20Mathematical%20Introduction%20to%20Compressive%20Sensing.pdf) for a short proof.
Note that this can be equvalently written as
$$
\infnorm{vec(\A^T\A - \I_{d})} \leq M
$$
2. **Restricted Isometry Property (RIP)** - Instead of enforcing an element-wise restcriction on the off-diagonals of the matrix $$\A^T\A$$, RIP (at a level $$s$$) requries a more general assumption that all $$s$$-sized principal block sub-matrices of $$\A^T\A$$ are $$\delta_s$$ close to $$\I_s$$ in the operator norm, i.e.,
$$
\enorm{\A_\Sset^T\A_\Sset-\I_s} \leq \delta_s\\
\text{or, } (1-\delta_s)\esqnorm{\vvec} \leq \esqnorm{\A\vvec} \leq (1+\delta_s)\esqnorm{\vvec} \qquad \forall\ \vvec \in\Rd{d} \text{ s.t. } \lznorm{\vvec} \leq s
$$


[Presentation](){:target="_blank"}