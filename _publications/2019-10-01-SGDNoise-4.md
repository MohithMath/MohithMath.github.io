---
title: "Non-Gaussianity of Stochastic Gradient Noise"
collection: publications
permalink: /publications/2019-10-01-SGDNoise-4
excerpt: 'We study the distribution of the Stochastic Gradient Noise during the training and observe that for batch sizes $$256$$ and above, the distribution is best described as Gaussian at-least in the early phases of training.'
date: 2019-10-01
venue: 'Science meets Engineering of Deep Learning (SEDL) workshop, Neural Information Processing Systems (NeurIPS)'
year: '2019'
authors: 'Abhishek Panigrahi, Raghav Somani, Navin Goyal & Praneeth Netrapalli'
arxiv: 'https://arxiv.org/abs/1910.09626'
bib: 'https://scholar.googleusercontent.com/scholar.bib?q=info:Rtg9joVlkxoJ:scholar.google.com/&output=citation&scisdr=CgWZbIysEIK-h1Izsn8:AAGBfm0AAAAAXb42qn_0t0wJ_7U1ZRs1KRs77YjqquCu&scisig=AAGBfm0AAAAAXb42qkM0sxEo-vpsCBeNHKGjsp24kdHQ&scisf=4&ct=citation&cd=-1&hl=en'

---
What enables Stochastic Gradient Descent (SGD) to achieve better generalization than Gradient Descent (GD) in Neural Network training? This question has attracted much attention. In this paper, we study the distribution of the Stochastic Gradient Noise (SGN) vectors during the training. We observe that for batch sizes $$256$$ and above, the distribution is best described as Gaussian at-least in the early phases of training. This holds across data-sets, architectures, and other choices.

The paper has been accepted at the [SEDL workshop](https://sites.google.com/view/sedl-neurips-2019/){:target="_blank"} at [NeurIPS 2019](https://nips.cc/Conferences/2019){:target="_blank"}.

Please find the below resources
1. [ArXiv](https://arxiv.org/pdf/1910.09626.pdf){:target="_blank"}.
