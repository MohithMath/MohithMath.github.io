---
title: "Memorization in Deep Neural Networks: Does the Loss Function matter?"
collection: publications
permalink: /publications/2020-07-12-role-of-loss
excerpt: 'The paper is under review.'
date: 2020-07-12
year: '2020'
authors: 'Deep Patel, PS Sastry'

---

<!-- bib: 'https://dl.acm.org/downformats.cfm?id=3291005&parent_id=3289600&expformat=bibtex' -->
<!-- code: 'https://github.com/dbp1994/mem-PAKDD' -->

> The paper is under review.

Abstract: Deep Neural Networks, often owing to the overparameterization, have shown to be capable of exactly memorizing even randomly labelled data. Empirical studies have also shown that none of the standard regularization techniques mitigate such overfitting. We investigate whether choice of loss function can affect this memorization. We empirically show, with benchmark data sets MNIST and CIFAR-10, that  a symmetric loss function as opposed to either cross entropy or squared error loss results in significant improvement in the ability of the network to resist such overfitting. We then provide a formal definition for robustness to memorization and provide theoretical explanation as to why the symmetric losses provide this robustness. Our results clearly bring out the role loss functions alone can play in this phenomenon of memorization.


<!-- Relevant links: -->
<!-- 1. [Paper](https://www.isca-speech.org/archive/Interspeech_2019/abstracts/1285.html){:target="_blank"}. -->
<!-- 2. [Poster](https://dbp1994.github.io/publications/files/Interspeech_ALS_2019_poster.pdf){:target="_blank"}. -->

<!--
<iframe width="560" height="315" src="https://www.youtube.com/embed/KyHUan_7YnQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<figcaption>Oral presentation at WSDM'19</figcaption> -->
